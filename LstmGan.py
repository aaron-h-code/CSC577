import torch
import torch.optim as optim
import torch.nn as nn
from torch.autograd import Variable

from tqdm import tqdm, trange
import numpy as np

from networks.Summarizer import Summarizer
from networks.Discriminator import Discriminator
from tools.LogWriter import LogWriter

import pdb

class LstmGan(object):
    def __init__(self, config = None, train_loader = None, test_loader = None, gt_loader = None):
        self.config = config
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.gt_loader = gt_loader

    # build lstm-gan model
    def build(self):
        # build Summarizier
        self.summarizer = Summarizer(self.config.input_size, 
                self.config.hidden_size, self.config.num_layers).cuda()
        # build Discriminator
        self.discriminator = Discriminator(self.config.input_size,
                self.config.hidden_size, self.config.num_layers).cuda()
        # hold and register submodules in a list
        self.model = nn.ModuleList([self.summarizer, self.discriminator])
        
        if self.config.mode == 'train':
            # build optimizers
            self.s_e_optimizer = optim.Adam(
                    list(self.summarizer.slstm.parameters()) + 
                    list(self.summarizer.vae.elstm.parameters()),
                    lr = self.config.sum_learning_rate)
            self.d_optimizer = optim.Adam(
                    list(self.summarizer.vae.dlstm.parameters()),
                    lr = self.config.sum_learning_rate)
            self.c_optimizer = optim.Adam(
                    list(self.discriminator.parameters()),
                    lr = self.config.dis_learning_rate)
            
            # set the model in training mode
            self.model.train()
            
            # initialize log writer
            self.writer = LogWriter(self.config.log_dir)

    """
    reconstruct loss
    Args:
        fea_h_last: given original vodieo feature, the output of the last hidden (top) layer of cLSTM, (1, hidden) = (1, 1024)
        dec_h_last: given decoded video feature, the output of the last hidden (top) layer of cLSTM, (1, hidden) = (1, 1024)
    """
    def get_reconst_loss(self, fea_h_last, dec_h_last):
        # L-2 norm
        return torch.norm(fea_h_last - dec_h_last, p = 2)
   
    """
    summary-length regularization
    Args:
        scores: (seq_len, 1) 
    """
    def get_sparsity_loss(self, scores):
        # convert scores to 0-1
        #scores = scores.ge(0.5).float()
        print(scores)
        return torch.abs(torch.mean(scores) - self.config.summary_rate)

    """
    dpp loss
    """
    def get_dpp_loss(self, scores):
        # convert scores to 0-1
        #scores = scores.ge(0.5).float()
        seq_len = len(scores)
        dpp_loss = 0
        for i in range((seq_len - 1)):
            dpp_loss += (scores[i] * scores[i + 1] + (1 - scores[i]) * (1 - scores[i + 1]))
        
        return torch.log(1 + dpp_loss)

    """
    gan loss of cLSTM
    Args:
        fea_prob: a scalar of original video feature
        dec_prob: a scalar of keyframe-based reconstruction video feature
        rand_dec_prob: a scalar of random-frame-based reconstruction video feature
    """
    def get_gan_loss(self, fea_prob, dec_prob, rand_dec_prob):
        gan_loss = torch.log(fea_prob) + torch.log(1 - dec_prob) + torch.log(1 - rand_dec_prob)
        
        return gan_loss    
    
    # train model
    def train(self):
        step = 0

        for epoch_i in range(self.config.n_epochs):
        #for epoch_i in trange(self.config.n_epochs, desc = 'Epoch'):
            s_e_loss_history = []
            d_loss_history = []
            c_loss_history = []
            # one video is a batch
            for batch_i, feature in enumerate(tqdm(self.train_loader,
                    desc = 'Batch', ncols = 240, leave = False)):
                
                # feature: (1, seq_len, input_size) -> (seq_len, 1, 2048)
                feature = feature.view(-1, 1, self.config.input_size)
                
                # from cpu tensor to gpu Variable
                feature = Variable(feature).cuda()
                
                """ train sLSTM and eLSTM """
                if self.config.detail_flag:
                    tqdm.write('------Training sLSTM and eLSTM------')
               
                # decoded: decoded feature generated by dLSTM
                scores, decoded = self.summarizer(feature)

                #scores.masked_fill_(scores >= 0.5, 1)
                #scores.masked_fill_(scores < 0.5, 0)

                # shape of feature and decoded: (seq_len, 1, 2048)
                # shape of fea_h_last and dec_h_last: (1, hidden_size) = (1, 2048)
                fea_h_last, fea_prob = self.discriminator(feature)
                dec_h_last, dec_prob = self.discriminator(decoded)

                tqdm.write('fea_prob: %.3f, dec_prob: %.3f}' % (fea_prob.data[0], dec_prob.data[0]))

                # reconstruction loss
                reconst_loss = self.get_reconst_loss(fea_h_last, dec_h_last)
                tqdm.write('reconst_loss: %.3f' % reconst_loss.data[0])
                
                # sparsity loss
                sparsity_loss = self.get_sparsity_loss(scores)
                tqdm.write('sparsity_loss: %.3f' % sparsity_loss.data[0])

                # dpp loss
                dpp_loss = self.get_dpp_loss(scores)
                tqdm.write('diversity_loss: %.3f' % dpp_loss.data[0])
                
                # minimize
                s_e_loss = reconst_loss + sparsity_loss + dpp_loss
                
                self.s_e_optimizer.zero_grad()
                s_e_loss.backward(retain_graph = True)
                self.s_e_optimizer.step()

                # add to loss history
                s_e_loss_history.append(s_e_loss.data.cpu())

                """ train dLSTM """
                if self.config.detail_flag:
                    tqdm.write('------Training dLSTM------')
                
                # randomly select a subset of frames
                _, rand_decoded = self.summarizer(feature, random_score_flag = True)
                # shape of rand_dec_h_last: (1, hidden_size) = (1, 2048)
                rand_dec_h_last, rand_dec_prob = self.discriminator(rand_decoded) 
                # gan loss
                gan_loss = self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)
                tqdm.write('gan_loss: %.3f' % gan_loss.data[0]) 

                # minimize
                d_loss = reconst_loss + gan_loss

                self.d_optimizer.zero_grad()
                d_loss.backward(retain_graph = True)
                self.d_optimizer.step()

                # add to loss history
                d_loss_history.append(d_loss.data.cpu())

                """ train cLSTM """
                if batch_i > self.config.dis_start_batch:
                    if self.config.detail_flag:
                        tqdm.write('------Training cLSTM------')

                # maximize
                c_loss = -1 * self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)

                self.c_optimizer.zero_grad()
                c_loss.backward()
                self.c_optimizer.step()
                 
                # add to loss history
                c_loss_history.append(c_loss.data.cpu()) 

                if self.config.detail_flag:
                    tqdm.write('------plotting------')

                self.writer.update_loss(reconst_loss.cpu().data.numpy(), step, 'reconst_loss')
                self.writer.update_loss(sparsity_loss.cpu().data.numpy(), step, 'sparsity_loss')
                self.writer.update_loss(gan_loss.cpu().data.numpy(), step, 'gan_loss')

                self.writer.update_loss(fea_prob.cpu().data.numpy(), step, 'fea_prob')
                self.writer.update_loss(dec_prob.cpu().data.numpy(), step, 'dec_prob')
                self.writer.update_loss(rand_dec_prob.cpu().data.numpy(), step, 'rand_dec_prob')

                # bacth over
                step += 1
            
            # epoch
            s_e_loss = torch.stack(s_e_loss_history).mean()
            d_loss = torch.stack(d_loss_history).mean()
            c_loss = torch.stack(c_loss_history).mean()
            
            # plot
            if self.config.detail_flag:
                tqdm.write('------plotting------')
            self.writer.update_loss(s_e_loss, epoch_i, 's_e_epoch')
            self.writer.update_loss(d_loss, epoch_i, 'd_loss_epoch')
            self.writer.update_loss(c_loss, epoch_i, 'c_loss_epoch')
    
            # save parameters at checkpoint
            model_path = str(self.config.model_save_dir) + '/' + 'model.pkl'
            tqdm.write('------save model parameters at %s' % model_path)
            torch.save(self.model.state_dict(), model_path)

    def test(self):
        model_path = str(self.config.model_save_dir) + '/' + 'model.pkl'
        # load model
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()

        ground_truth = self.gt_loader

        pred = {}
        cnt = 0
        precision = []
        for sample_i, feature in enumerate(tqdm(self.test_loader, desc = 'Evaluate', leave = False)):
            # feautre: (seq_len, 1, 2048)
            feature = feature.view(-1, 1, self.config.input_size)
            scores = self.summarizer.slstm(Variable(feature.cuda(), volatile = True))
            # convert scores to a list
            scores = scores.data.cpu().numpy().flatten()
            pred[sample_i] = scores

            print(scores)
            
            # sort socres in descending order, and return index
            idx_sorted_scores = sorted(range(len(scores)), key = lambda k : scores[k], reverse = True)
    
            num_keyframes = int(len(scores) * self.config.summary_rate)
            idx_keyframes = idx_sorted_scores[:num_keyframes]
            # sort key frames index in ascending order
            idx_keyframes = sorted(idx_keyframes)

            # precision for a single video
            single_precision = float(len(set(ground_truth[cnt]) & set(idx_keyframes))) / float(len(ground_truth[cnt]))

            # add single video precision to the whole precision list
            precision.append(single_precision)
               
            cnt += 1
        
        # average precision for the dataset
        avg_precision = sum(precision) / float(len(precision))

