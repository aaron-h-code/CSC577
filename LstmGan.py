import torch
import torch.optim as optim
import torch.nn as nn
from torch.autograd import Variable

from tqdm import tqdm, trange

from networks.Summarizer import Summarizer
from networks.Discriminator import Discriminator
from tools.LogWriter import LogWriter

import pdb

class LstmGan(object):
    def __init__(self, config = None, train_loader = None, test_loader = None):
        self.config = config
        self.train_loader = train_loader
        self.text_loader = test_loader

    # build lstm-gan model
    def build(self):
        # build Summarizier
        self.summarizer = Summarizer(self.config.input_size, 
                self.config.hidden_size, self.config.num_layers).cuda()
        # build Discriminator
        self.discriminator = Discriminator(self.config.input_size,
                self.config.hidden_size, self.config.num_layers).cuda()
        # hold and register submodules in a list
        self.model = nn.ModuleList([self.summarizer, self.discriminator])
        
        if self.config.mode == 'train':
            # build optimizers
            self.s_e_optimizer = optim.Adam(
                    list(self.summarizer.slstm.parameters()) + 
                    list(self.summarizer.vae.elstm.parameters()),
                    lr = self.config.sum_learning_rate)
            self.d_optimizer = optim.Adam(
                    list(self.summarizer.vae.dlstm.parameters()),
                    lr = self.config.sum_learning_rate)
            self.c_optimizer = optim.Adam(
                    list(self.discriminator.parameters()),
                    lr = self.config.dis_learning_rate)
            
            # set the model in training mode
            self.model.train()
            
            # initialize log writer
            self.writer = LogWriter(self.config.log_dir)

    """
    reconstruct loss
    Args:
        fea_h_last: given original vodieo feature, the output of the last hidden (top) layer of cLSTM, (1, hidden) = (1, 1024)
        dec_h_last: given decoded video feature, the output of the last hidden (top) layer of cLSTM, (1, hidden) = (1, 1024)
    """
    def get_reconst_loss(self, fea_h_last, dec_h_last):
        # L-2 norm
        return torch.norm(fea_h_last - dec_h_last, p = 2)
    
    """
    summary-length regularization
    Args:
        scores: (seq_len, 1) 
    """
    def get_sparsity_loss(self, scores):
        return torch.abs(torch.mean(scores) - self.config.summary_rate)

    """
    gan loss of cLSTM
    Args:
        fea_prob: a scalar of original video feature
        dec_prob: a scalar of keyframe-based reconstruction video feature
        rand_dec_prob: a scalar of random-frame-based reconstruction video feature
    """
    def get_gan_loss(self, fea_prob, dec_prob, rand_dec_prob):
        gan_loss = torch.log(fea_prob) + torch.log(1 - dec_prob) + torch.log(1 - rand_dec_prob)
        
        return gan_loss    
    
    # train model
    def train(self):
        step = 0

        for epoch_i in range(self.config.n_epochs):
        #for epoch_i in trange(self.config.n_epochs, desc = 'Epoch'):
            s_e_loss_history = []
            d_loss_history = []
            c_loss_history = []
            # one video is a batch
            for batch_i, feature in enumerate(tqdm(self.train_loader,
                    desc = 'Batch', ncols = 240, leave = False)):
                
                # feature: (1, seq_len, input_size) -> (seq_len, 1, 2048)
                feature = feature.view(-1, 1, self.config.input_size)
                
                # from cpu tensor to gpu Variable
                feature = Variable(feature).cuda()
                
                """ train sLSTM and eLSTM """
                if self.config.detail_flag:
                    tqdm.write('Training sLSTM and eLSTM')
               
                # decoded: decoded feature generated by dLSTM
                scores, decoded = self.summarizer(feature)

                # shape of feature and decoded: (seq_len, 1, 2048)
                # shape of fea_h_last and dec_h_last: (1, hidden_size) = (1, 2048)
                fea_h_last, fea_prob = self.discriminator(feature)
                dec_h_last, dec_prob = self.discriminator(decoded)

                # reconstruction loss
                reconst_loss = self.get_reconst_loss(fea_h_last, dec_h_last)
                print(reconst_loss)
                
                # sparsity loss
                sparsity_loss = self.get_sparsity_loss(scores)
                print(sparsity_loss)
                
                # minimize
                s_e_loss = reconst_loss + sparsity_loss
                
                self.s_e_optimizer.zero_grad()
                s_e_loss.backward(retain_graph = True)
                self.s_e_optimizer.step()

                # add to loss history
                s_e_loss_history.append(s_e_loss.data.cpu())

                """ train dLSTM """
                if self.config.detail_flag:
                    tqdm.write('Training dLSTM')
                
                # randomly select a subset of frames
                _, rand_decoded = self.summarizer(feature, random_score_flag = True)
                # shape of rand_dec_h_last: (1, hidden_size) = (1, 2048)
                rand_dec_h_last, rand_dec_prob = self.discriminator(rand_decoded) 
                # gan loss
                gan_loss = self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)
                print(gan_loss) 

                # minimize
                d_loss = reconst_loss + gan_loss

                self.d_optimizer.zero_grad()
                d_loss.backward(retain_graph = True)
                self.d_optimizer.step()

                # add to loss history
                d_loss_history.append(d_loss.data.cpu())

                """ train cLSTM """
                if batch_i > self.config.dis_start_batch:
                    if self.config.detail_flag:
                        tqdm.write('Training cLSTM')

                # maximize
                c_loss = -1 * self.get_gan_loss(fea_prob, dec_prob, rand_dec_prob)

                self.c_optimizer.zero_grad()
                c_loss.backward()
                self.c_optimizer.step()
                 
                # add to loss history
                c_loss_history.append(c_loss.data.cpu()) 

                # bacth over
                step += 1
            
            # epoch
            s_e_loss = torch.stack(s_e_loss_history).mean()
            d_loss = torch.stack(d_loss_history).mean()
            c_loss = torch.stack(c_loss_history).mean()
            
            print(s_e_loss)
            print(d_loss)
            print(c_loss)

